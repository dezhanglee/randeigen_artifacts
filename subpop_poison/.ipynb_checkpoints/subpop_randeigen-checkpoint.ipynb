{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637cb3c3-86aa-4b77-b53b-a9fb1305c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import ConvNet\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d91fa-0710-4fb6-a219-21eec6596cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "# batch_size = len(train_set) // args.nworker\n",
    "train_loader = DataLoader(train_set)\n",
    "test_loader = DataLoader(torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform))\n",
    "\n",
    "network = ConvNet(input_size=28, input_channel=1, classes=10, filters1=30, filters2=30, fc_size=200).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f248b736-0e4b-48a7-934e-6dc24f36ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=4, out_channels=10, kernel_size=5, stride=1)\n",
    "        self.fc_1 = nn.Linear(in_features=4 * 4 * 10, out_features=100)\n",
    "        self.fc_2 = nn.Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 10)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa4a5ca-8943-4b7c-a2ea-f49c6c4dd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3addd8be-43d9-48e2-8890-6afb2f46ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poisoning rate\n",
    "alpha = 2\n",
    "# alpha=0\n",
    "\n",
    "# do clustering to get a subpop\n",
    "k=100 # recommended param in paper\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "x_train = torch.Tensor(np.array([i[0][0] for i in train_loader])).to(device)\n",
    "y_train = torch.Tensor(np.array([i[1][0] for i in train_loader])).type(torch.long).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bd6b3a-9216-4cfc-a53d-504d24478379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params (LR) recommended in paper\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52abb6df-9b47-4955-854a-f8489c69a183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "epoch 40\n",
      "epoch 41\n",
      "epoch 42\n",
      "epoch 43\n",
      "epoch 44\n",
      "epoch 45\n",
      "epoch 46\n",
      "epoch 47\n",
      "epoch 48\n",
      "epoch 49\n"
     ]
    }
   ],
   "source": [
    "# train a base model with 20% of the training data, then we fine tune on the poisoned data\n",
    "\n",
    "n_epoch = 50\n",
    "batch_size = 128\n",
    "train_set_size = len(x_train)//5\n",
    "for _ in range(n_epoch):\n",
    "    print(\"epoch \" + str(_))\n",
    "    \n",
    "    for i in range(0,train_set_size,batch_size):\n",
    "\n",
    "    \n",
    "        feature = x_train[i:i+batch_size]\n",
    "        feature.requires_grad = True  ### CRUCIAL LINE !!!\n",
    "        target = y_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output = network(feature)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0908e5-7faa-46a6-a0e5-665edfa62e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_set_size:]\n",
    "y_train = y_train[train_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718c5320-e1c3-49b2-af0a-d1e50b070327",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = [i.flatten().cpu() for i in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb92d89e-09e6-48eb-8b15-ef39df6c3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels  = kmeans.fit_predict(x_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcfdcf0-a1e1-4ebc-8460-fcce894713bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = dict()\n",
    "for i in cluster_labels:\n",
    "    cluster_count[i] = cluster_count.get(i,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b10fc91-f914-4c1f-b35f-e08ed3c6f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poison 5 smallest subpopulations\n",
    "poison_cluster = sorted([i for i in cluster_count.items()], key=lambda x:x[1])[:5]\n",
    "n_poison_each_cluster = dict([(i[0], int(i[1]*alpha)) for i in poison_cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73aa9c7e-138b-4fd3-81c1-7f83a63c22eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{46: 544, 13: 550, 77: 554, 94: 562, 76: 586}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_poison_each_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252b3a59-0272-424d-add2-c7b8ff917726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(46, 272), (13, 275), (77, 277), (94, 281), (76, 293)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poison_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b6b8f39-bc87-4077-961a-9c80621f9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "new_x = []\n",
    "new_y = []\n",
    "new_y_correct=[]\n",
    "for i in range(len(y_train)):\n",
    "    cluster_y = cluster_labels[i]\n",
    "    if cluster_y in n_poison_each_cluster and n_poison_each_cluster[cluster_y] > 0:\n",
    "        for _ in range(alpha):\n",
    "            new_y.append((y_train[i]+random.randint(1,9))%10)\n",
    "            new_x.append(x_train[i])\n",
    "            new_y_correct.append(y_train[i])\n",
    "            n_poison_each_cluster[cluster_y] = n_poison_each_cluster[cluster_y] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8172fd-8e56-4844-a824-f4be05d241c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_poisoned = torch.concat((x_train, torch.Tensor( np.array([i.cpu() for i in new_x])).to(device)))\n",
    "y_train_poisoned = torch.concat((y_train, torch.Tensor( np.array([i.cpu() for i in new_y])).to(device)))\n",
    "y_train_correct =  torch.concat((y_train, torch.Tensor( np.array([i.cpu() for i in new_y_correct])).to(device)))\n",
    "# y_train_poisoned = y_train + new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a04ec384-13c0-469c-914c-727b59d993cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_poisoned = x_train_poisoned.to(device)\n",
    "y_train_poisoned = y_train_poisoned.type(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12593e45-30f5-431f-be4a-01a672fb4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [i for i in range(len(x_train_poisoned))]\n",
    "import random\n",
    "random.seed(0)\n",
    "train_idx_shuffle = random.shuffle(train_idx)\n",
    "x_train_poisoned = x_train_poisoned[train_idx_shuffle][0]\n",
    "y_train_poisoned = y_train_poisoned[train_idx_shuffle][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578cd0e-531c-4425-9eb1-d7abe112ff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd0dc33-a969-417f-9fb8-d244476eee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48000, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train_poisoned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8495d8-e798-473f-8894-7bf6911683cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99fafc76-57df-4d4c-af9e-44e18c74845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which of the training data falls into the cluster\n",
    "x_test = [i[0] for i in test_loader]\n",
    "y_test = [i[1] for i in test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de43607-abcc-4e28-be6b-a4fb4d8bcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of randeigen\n",
    "# instead of giving a robust aggregate over the sample, we return the samples which are removed\n",
    "# take in a set of inputs and their corresponding gradient\n",
    "# output the clean inputs \n",
    "import math\n",
    "\n",
    "def power_iteration(mat, iterations, device):\n",
    "    dim = mat.shape[0]\n",
    "    u = torch.randn((dim, 1)).to(device)\n",
    "    for _ in range(iterations):\n",
    "        u = mat @ u / torch.linalg.norm(mat @ u) \n",
    "    eigenvalue = u.T @ mat @ u\n",
    "    return eigenvalue, u\n",
    "\n",
    "# return the index of the clean samples\n",
    "def randomized_agg_forced(data, eps_poison=0.2, eps_jl=0.1, eps_pow = 0.1, seed=12):\n",
    "    n = int(data.shape[0])\n",
    "    feature_shape = data[0].shape\n",
    "    n_dim = int(np.prod(np.array(feature_shape)))\n",
    "    res =  _randomized_agg(data, eps_poison, eps_jl, eps_pow, 1, 10**-5, forced=True, seed=seed) # set threshold for convergence as 1*10**-5 (i.e. float point error)\n",
    "    return res\n",
    "\n",
    "def _randomized_agg(data, eps_poison=0.2, eps_jl=0.1, eps_pow = 0.1, threshold = 20, clean_eigen = 10**-5, forced=False, seed=None):\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    n = int(data.shape[0])\n",
    "    data = data.to(device)\n",
    "    \n",
    "    d = int(math.prod(data[0].shape))\n",
    "    data_flatten = data.reshape(n, d)\n",
    "    data_mean = torch.mean(data_flatten, dim=0)\n",
    "    data_sd = torch.std(data_flatten, dim=0)\n",
    "    data_norm = (data_flatten - data_mean)/data_sd\n",
    "    \n",
    "    k = min(int(math.log(d)//eps_jl**2), d)\n",
    "    \n",
    "    A = torch.randn((d, k)).to(device)\n",
    "    A = A/(k**0.5)\n",
    "\n",
    "    Y = data_flatten @ A # n times k\n",
    "    Y = Y.to(device)\n",
    "    # print(k)\n",
    "    power_iter_rounds = int(- math.log(4*k)/(2*math.log(1-eps_pow)))\n",
    "    clean_eigen = clean_eigen * d/k\n",
    "    old_eigenvalue = None\n",
    "    for _ in range(max(int(eps_poison*n), 10)):\n",
    "        Y_mean = torch.mean(Y, dim=0)\n",
    "        # Y = (Y - Y_mean)\n",
    "        Y_cov = torch.cov(Y.T)\n",
    "        Y_sq = Y_cov\n",
    "        # print(Y_sq)\n",
    "        eigenvalue, eigenvector = power_iteration(Y_sq, power_iter_rounds, device)\n",
    "\n",
    "        proj_Y = torch.abs(Y @ eigenvector )\n",
    "        proj_Y = torch.flatten(proj_Y)\n",
    "        if forced and old_eigenvalue and abs(old_eigenvalue - eigenvalue) < 10**-5:\n",
    "            # print('converge')\n",
    "            break\n",
    "\n",
    "        if len(proj_Y) < eps_poison*n or sum([i > 0.5 for i in proj_Y/torch.max(proj_Y)]) > len(proj_Y)*(1-2*eps_poison):\n",
    "            # print('new_criteria')\n",
    "            break \n",
    "        old_eigenvalue = eigenvalue\n",
    "        \n",
    "        uniform_rand = torch.rand(proj_Y.shape).to(device)\n",
    "        kept_idx = uniform_rand > (proj_Y/torch.max(proj_Y))\n",
    "        Y = Y[kept_idx]\n",
    "        data = data[kept_idx]\n",
    "    return kept_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "800e0851-2eae-4187-b63b-14e63f112c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poisoned_flat = [i.flatten().cpu() for i in x_train_poisoned]\n",
    "poisoned_cluster_labels = kmeans.predict(x_poisoned_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ac23ea-0859-4ea1-9f0b-f11efbc27254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0.9090382387022016 0.05722460658082973\n",
      "epoch 1\n",
      "0.9116346937899661 0.05507868383404868\n",
      "epoch 2\n",
      "0.9147676065404918 0.05579399141630903\n",
      "epoch 3\n",
      "0.9170421870305996 0.05507868383404868\n",
      "epoch 4\n",
      "0.9189948929230505 0.05507868383404868\n",
      "epoch 5\n",
      "0.9207973906699284 0.05364806866952787\n",
      "epoch 6\n",
      "0.9227500965623793 0.05364806866952787\n",
      "epoch 7\n",
      "0.9244882193897258 0.05078683834048636\n",
      "epoch 8\n",
      "0.9267842581863439 0.05221745350500717\n",
      "epoch 9\n",
      "0.9282863396420754 0.05078683834048636\n",
      "epoch 10\n",
      "0.9296596712587443 0.05007153075822601\n",
      "epoch 11\n",
      "0.9315694605381744 0.048640915593705314\n",
      "epoch 12\n",
      "0.9330500836873954 0.048640915593705314\n",
      "epoch 13\n",
      "0.9346379983691687 0.047925608011444965\n",
      "epoch 14\n",
      "0.9358611218402644 0.047925608011444965\n",
      "epoch 15\n",
      "0.9368696622462556 0.047210300429184504\n",
      "epoch 16\n",
      "0.9377923694262049 0.046494992846924155\n",
      "epoch 17\n",
      "0.9388867430582378 0.045779685264663805\n",
      "epoch 18\n",
      "0.930088408222823 0.045779685264663805\n",
      "epoch 19\n",
      "0.9312900733874082 0.045064377682403456\n",
      "epoch 20\n",
      "0.9320196558087636 0.044349070100143106\n",
      "epoch 21\n",
      "0.9331998626668383 0.043633762517882646\n",
      "epoch 22\n",
      "0.9341440281532981 0.0429184549356223\n",
      "epoch 23\n",
      "0.9350023604137161 0.0429184549356223\n",
      "epoch 24\n",
      "0.9357319428350714 0.0429184549356223\n",
      "epoch 25\n",
      "0.936375692030385 0.043633762517882646\n",
      "epoch 26\n",
      "0.9373627741298657 0.043633762517882646\n",
      "epoch 27\n",
      "0.9378992317926269 0.0414878397711016\n",
      "epoch 28\n",
      "0.9388863138921076 0.0414878397711016\n",
      "epoch 29\n",
      "0.939615896313463 0.04077253218884125\n",
      "epoch 30\n",
      "0.9302167288957556 0.04077253218884125\n",
      "epoch 31\n",
      "0.9309677696236213 0.0414878397711016\n",
      "epoch 32\n",
      "0.9316544354319557 0.0414878397711016\n",
      "epoch 33\n",
      "0.9324698510793528 0.04005722460658079\n",
      "epoch 34\n",
      "0.9330277670486246 0.04077253218884125\n",
      "epoch 35\n",
      "0.9336285996309172 0.04005722460658079\n",
      "epoch 36\n",
      "0.9338431826960216 0.03934191702432044\n",
      "epoch 37\n",
      "0.9344010986652933 0.03934191702432044\n",
      "epoch 38\n",
      "0.9349804729410755 0.03934191702432044\n",
      "epoch 39\n",
      "0.9352808892322218 0.03934191702432044\n",
      "epoch 40\n",
      "0.9358817218145144 0.03862660944206009\n",
      "epoch 41\n",
      "0.9363323462512339 0.03862660944206009\n",
      "epoch 42\n",
      "0.9369760954465474 0.03862660944206009\n",
      "epoch 43\n",
      "0.9375125531093086 0.03791130185979974\n",
      "epoch 44\n",
      "0.9378344277069654 0.03719599427753939\n",
      "epoch 45\n",
      "0.938435260289258 0.03719599427753939\n",
      "epoch 46\n",
      "0.9386927599673833 0.03791130185979974\n",
      "epoch 47\n",
      "0.9392721342431655 0.03791130185979974\n",
      "epoch 48\n",
      "0.9395510922278013 0.03791130185979974\n",
      "epoch 49\n",
      "0.9300017166645208 0.03791130185979974\n",
      "epoch 50\n",
      "0.9303235912621776 0.03791130185979974\n",
      "epoch 51\n",
      "0.9306454658598343 0.03791130185979974\n",
      "epoch 52\n",
      "0.9310317153770225 0.03719599427753939\n",
      "epoch 53\n",
      "0.931439423200721 0.03648068669527893\n",
      "epoch 54\n",
      "0.9316540062658255 0.03648068669527893\n",
      "epoch 55\n",
      "0.9319758808634823 0.03648068669527893\n",
      "epoch 56\n",
      "0.9324694219132226 0.03576537911301858\n",
      "epoch 57\n",
      "0.932726921591348 0.03505007153075823\n",
      "epoch 58\n",
      "0.9330487961890047 0.03576537911301858\n",
      "epoch 59\n",
      "0.933392129093172 0.03576537911301858\n",
      "epoch 60\n",
      "0.9337140036908287 0.03576537911301858\n",
      "epoch 61\n",
      "0.9340573365949959 0.03505007153075823\n",
      "epoch 62\n",
      "0.9342504613535899 0.03505007153075823\n",
      "epoch 63\n",
      "0.934486502725205 0.03433476394849788\n",
      "epoch 64\n",
      "0.9346152525642676 0.03648068669527893\n",
      "epoch 65\n",
      "0.9347654607098408 0.03576537911301858\n",
      "epoch 66\n",
      "0.9349585854684348 0.03576537911301858\n",
      "epoch 67\n",
      "0.9351731685335393 0.03505007153075823\n",
      "epoch 68\n",
      "0.9356881678897902 0.03505007153075823\n",
      "epoch 69\n",
      "0.9359027509548946 0.03505007153075823\n",
      "epoch 70\n",
      "0.93616025063302 0.03433476394849788\n",
      "epoch 71\n",
      "0.9364392086176559 0.03433476394849788\n",
      "epoch 72\n",
      "0.9366752499892709 0.03433476394849788\n",
      "epoch 73\n",
      "0.9366967082957812 0.03361945636623753\n",
      "epoch 74\n",
      "0.9369756662804172 0.03361945636623753\n",
      "epoch 75\n",
      "0.9372760825715635 0.03361945636623753\n",
      "epoch 76\n",
      "0.9374048324106261 0.03290414878397707\n",
      "epoch 77\n",
      "0.9375550405561993 0.03290414878397707\n",
      "epoch 78\n",
      "0.9377052487017724 0.03361945636623753\n",
      "epoch 79\n",
      "0.938112956525471 0.03290414878397707\n",
      "epoch 80\n",
      "0.9382631646710441 0.03361945636623753\n",
      "epoch 81\n",
      "0.9384777477361487 0.03361945636623753\n",
      "epoch 82\n",
      "0.9385206643491696 0.03290414878397707\n",
      "epoch 83\n",
      "0.9387137891077636 0.03218884120171672\n",
      "epoch 84\n",
      "0.9390142053989099 0.03218884120171672\n",
      "epoch 85\n",
      "0.9393575383030771 0.03218884120171672\n",
      "epoch 86\n",
      "0.9395077464486502 0.03218884120171672\n",
      "epoch 87\n",
      "0.9397223295137548 0.03218884120171672\n",
      "epoch 88\n",
      "0.9399369125788593 0.03218884120171672\n",
      "epoch 89\n",
      "0.9300871207244324 0.03147353361945637\n",
      "epoch 90\n",
      "0.9304304536285996 0.030758226037196024\n",
      "epoch 91\n",
      "0.9305592034676623 0.030758226037196024\n",
      "epoch 92\n",
      "0.9307952448392773 0.030042918454935674\n",
      "epoch 93\n",
      "0.9310312862108922 0.030042918454935674\n",
      "epoch 94\n",
      "0.9311814943564654 0.030042918454935674\n",
      "epoch 95\n",
      "0.9313317025020385 0.030042918454935674\n",
      "epoch 96\n",
      "0.9315248272606326 0.029327610872675214\n",
      "epoch 97\n",
      "0.9316750354062057 0.029327610872675214\n",
      "epoch 98\n",
      "0.931782326938758 0.029327610872675214\n",
      "epoch 99\n",
      "0.9319325350843312 0.029327610872675214\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "import copy\n",
    "n_epoch = 100\n",
    "batch_size = 128\n",
    "for _ in range(n_epoch):\n",
    "    print(\"epoch \" + str(_))\n",
    "    \n",
    "    for i in range(0,len(x_train_poisoned),batch_size):\n",
    "\n",
    "        \n",
    "        network_copy = copy.deepcopy(network)\n",
    "        feature = x_train_poisoned[i:i+batch_size]\n",
    "        feature.requires_grad = True  ### CRUCIAL LINE !!!\n",
    "        target = y_train_poisoned[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output = network_copy(feature)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep the good entries only\n",
    "        # vv = [network_copy.conv_1.weight.grad, network_copy.conv_2.weight.grad, network_copy.fc_1.weight.grad, network_copy.fc_2.weight.grad]\n",
    "        # vv = [zz.flatten() for zz in vv]\n",
    "        # print(vv)\n",
    "        kept_idx = randomized_agg_forced(feature.grad.flatten(start_dim=-1))\n",
    "        # print(torch.mean(kept_idx))\n",
    "        kept_idx = [i for i in range(len(kept_idx)) if kept_idx[i]]\n",
    "        \n",
    "        # train the model with the kept data\n",
    "        feature = x_train_poisoned[i:i+batch_size][kept_idx]\n",
    "        target = y_train_poisoned[i:i+batch_size][kept_idx]\n",
    "        optimizer.zero_grad()\n",
    "        output = network(feature)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        ncorrect_clean = 0\n",
    "        ncorrect_poison = 0\n",
    "        \n",
    "        n_clean = 0\n",
    "        n_poison = 0\n",
    "        \n",
    "        out_prob = network(x_train_poisoned)\n",
    "        out_class = torch.argmax(out_prob, dim=-1)\n",
    "        for i in range(len(out_class)):\n",
    "            pred_class = int(out_class[i])\n",
    "            actual_class = int(y_train_correct[i])\n",
    "        \n",
    "            correct = int(pred_class == actual_class)\n",
    "        \n",
    "            if poisoned_cluster_labels[i] in n_poison_each_cluster:\n",
    "                ncorrect_poison += correct\n",
    "                n_poison += 1\n",
    "            else:\n",
    "                ncorrect_clean += correct\n",
    "                n_clean += 1\n",
    "        clean_acc = ncorrect_clean/n_clean\n",
    "        asr = 1-ncorrect_poison/n_poison # propotion we managed to flip wrongly\n",
    "        print(clean_acc, asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b789cdc-01e9-4aea-be1d-f2d19ebcb323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
