{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c73553-21e2-460c-a746-66ef3af21f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "device=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878453c8-6274-444b-94f2-35a340de0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: F.pad(\n",
    "            Variable(x.unsqueeze(0), requires_grad=False),\n",
    "            (4, 4, 4, 4), mode='reflect').data.squeeze()),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, #batch_size=batch_size,\n",
    "                                         )\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, #batch_size=batch_size,\n",
    "                                        )\n",
    "                                         #shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4e6fb2-3a38-445e-a848-c5e03082a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dezhang/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dezhang/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "net = net.to(device)\n",
    "# params (LR) recommended in paper\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1bbf94-b9fd-4e61-bc92-880ecf2b1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from art.estimators.classification import PyTorchClassifier\n",
    "# classifier = PyTorchClassifier(\n",
    "#     model=net,\n",
    "#     clip_values=(0, 1),\n",
    "#     loss=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     input_shape=(3,32,32),\n",
    "#     nb_classes=10,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf138a78-25a8-4071-a813-ef9586138f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gma = GradientMatchingAttack(classifier, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1c59b6-59c8-4189-a083-23e370eaf208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as T\n",
    "directory = os.fsencode(\"poisoned_cifar10/train/cat/\")\n",
    "poisoned_imgs = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".png\"): \n",
    "        a=torchvision.io.read_image(\"poisoned_cifar10/train/cat/\"+filename)\n",
    "        b=T.ToPILImage()(a) \n",
    "        poisoned_imgs.append(transform(b))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "poisoned_labels = torch.Tensor([3 for i in poisoned_imgs]).to(device)\n",
    "poisoned_imgs = torch.stack(poisoned_imgs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310bd836-c14a-460a-a490-88361c66566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as T\n",
    "directory = os.fsencode(\"poisoned_cifar10/targets/cat/\")\n",
    "poisoned_imgs_test = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".png\"): \n",
    "        a=torchvision.io.read_image(\"poisoned_cifar10/targets/cat/\"+filename)\n",
    "        b=T.ToPILImage()(a) \n",
    "        poisoned_imgs_test.append(transform(b))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "poisoned_labels_test = torch.Tensor([1 for i in poisoned_imgs_test]).to(device)\n",
    "poisoned_imgs_test = torch.stack(poisoned_imgs_test*2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380618aa-677c-4112-9290-4ec829ac9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(np.array([i[0][0] for i in trainloader])).to(device)\n",
    "y_train = torch.Tensor(np.array([i[1][0] for i in trainloader])).type(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588a58f8-630e-42aa-b772-33c6c5537f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # train a base model with 20% of the training data, then we fine tune on the poisoned data\n",
    "\n",
    "# n_epoch = 100\n",
    "# batch_size = 512\n",
    "# train_set_size = len(x_train)//2\n",
    "# for _ in range(n_epoch):\n",
    "#     print(\"epoch \" + str(_))\n",
    "    \n",
    "#     for i in range(0,train_set_size,batch_size):\n",
    "\n",
    "    \n",
    "#         feature = x_train[i:i+batch_size]\n",
    "#         # feature.requires_grad = True  ### CRUCIAL L?INE !!!\n",
    "#         target = y_train[i:i+batch_size]\n",
    "#         optimizer.zero_grad()\n",
    "#         output = net(feature)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         # non car\n",
    "#         train_prob = net(x_train[train_set_size:])\n",
    "#         train_pred = torch.argmax(train_prob, dim=-1)\n",
    "#         n_correct = torch.sum(train_pred == y_train[train_set_size:])\n",
    "#         # print(train_pred)\n",
    "#         print(\"TEST ACC:\", float(n_correct/train_set_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9449ff-ef18-492b-983a-9380939ca173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975b34cf-55a8-433a-a0d5-17348ac7b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add poisoned data\n",
    "\n",
    "x_train_poisoned = torch.concat([x_train, poisoned_imgs])\n",
    "y_train_poisoned = torch.concat([y_train, poisoned_labels]).type(torch.long).to(device)\n",
    "\n",
    "# x_train_poisoned=x_train\n",
    "# y_train_poisoned = y_train\n",
    "\n",
    "x_train_car = torch.stack([x_train_poisoned[i] for i in range(len(x_train_poisoned)) if y_train_poisoned[i] == 1]).to(device)\n",
    "x_train_not_car = torch.stack([x_train_poisoned[i] for i in range(len(x_train_poisoned)) if y_train_poisoned[i] != 1]).to(device)\n",
    "y_train_not_car = torch.stack([y_train_poisoned[i] for i in range(len(x_train_poisoned)) if y_train_poisoned[i] != 1]).to(device)\n",
    "# import random\n",
    "# random.seed(0)\n",
    "# shuffle_idx = random.shuffle([i for i in range(len(x_train_poisoned))])\n",
    "# x_train_poisoned = x_train_poisoned[shuffle_idx]\n",
    "# y_train_poisoned = y_train_poisoned[shuffle_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b0cd3-4702-4e1f-9286-01cb8997759f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72592e26-9881-4333-9f2f-e0963e1bfa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50500, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_poisoned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dbf6fb6-2e0b-44a5-907e-675fcb55f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test acc\n",
    "x_test = torch.stack([i[0][0] for i in testloader]).to(device)\n",
    "y_test = torch.stack([i[1][0] for i in testloader]).type(torch.long).to(device)\n",
    "# y_train_poisoned = torch.concat([y_train, poisoned_labels])[0].type(torch.long).to(device)\n",
    "\n",
    "x_test_car = torch.stack([x_test[i] for i in range(len(x_test)) if y_test[i] == 2]).to(device)\n",
    "x_test_not_car = torch.stack([x_test[i] for i in range(len(x_test)) if y_test[i] != 2]).to(device)\n",
    "y_test_not_car = torch.stack([y_test[i] for i in range(len(x_test)) if y_test[i] != 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4eacdd-8175-4a6b-8272-299780094959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_car.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e44fd5-6f84-40dc-ab8f-e5b9ee6fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of randeigen\n",
    "# instead of giving a robust aggregate over the sample, we return the samples which are removed\n",
    "# take in a set of inputs and their corresponding gradient\n",
    "# output the clean inputs \n",
    "import math\n",
    "\n",
    "def power_iteration(mat, iterations, device):\n",
    "    dim = mat.shape[0]\n",
    "    u = torch.randn((dim, 1)).to(device)\n",
    "    for _ in range(iterations):\n",
    "        u = mat @ u / torch.linalg.norm(mat @ u) \n",
    "    eigenvalue = u.T @ mat @ u\n",
    "    return eigenvalue, u\n",
    "\n",
    "# return the index of the clean samples\n",
    "def randomized_agg_forced(data, eps_poison=0.2, eps_jl=0.1, eps_pow = 0.1, seed=12):\n",
    "    n = int(data.shape[0])\n",
    "    feature_shape = data[0].shape\n",
    "    n_dim = int(np.prod(np.array(feature_shape)))\n",
    "    res =  _randomized_agg(data, eps_poison, eps_jl, eps_pow, 1, 10**-5, forced=True, seed=seed) # set threshold for convergence as 1*10**-5 (i.e. float point error)\n",
    "    return res\n",
    "\n",
    "def _randomized_agg(data, eps_poison=0.2, eps_jl=0.1, eps_pow = 0.1, threshold = 20, clean_eigen = 10**-5, forced=False, seed=None):\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    n = int(data.shape[0])\n",
    "    data = data.to(device)\n",
    "    \n",
    "    d = int(math.prod(data[0].shape))\n",
    "    data_flatten = data.reshape(n, d)\n",
    "    data_mean = torch.mean(data_flatten, dim=0)\n",
    "    data_sd = torch.std(data_flatten, dim=0)\n",
    "    data_norm = (data_flatten - data_mean)/data_sd\n",
    "    \n",
    "    k = min(int(math.log(d)//eps_jl**2), d)\n",
    "    \n",
    "    A = torch.randn((d, k)).to(device)\n",
    "    A = A/(k**0.5)\n",
    "\n",
    "    Y = data_flatten @ A # n times k\n",
    "    Y = Y.to(device)\n",
    "    # print(k)\n",
    "    power_iter_rounds = int(- math.log(4*k)/(2*math.log(1-eps_pow)))\n",
    "    clean_eigen = clean_eigen * d/k\n",
    "    old_eigenvalue = None\n",
    "    for _ in range(max(int(eps_poison*n), 10)):\n",
    "        Y_mean = torch.mean(Y, dim=0)\n",
    "        # Y = (Y - Y_mean)\n",
    "        Y_cov = torch.cov(Y.T)\n",
    "        Y_sq = Y_cov\n",
    "        # print(Y_sq)\n",
    "        eigenvalue, eigenvector = power_iteration(Y_sq, power_iter_rounds, device)\n",
    "\n",
    "        proj_Y = torch.abs(Y @ eigenvector )\n",
    "        proj_Y = torch.flatten(proj_Y)\n",
    "        if forced and old_eigenvalue and abs(old_eigenvalue - eigenvalue) < 10**-5:\n",
    "            # print('converge')\n",
    "            break\n",
    "\n",
    "        if len(proj_Y) < eps_poison*n or sum([i > 0.5 for i in proj_Y/torch.max(proj_Y)]) > len(proj_Y)*(1-2*eps_poison):\n",
    "            # print('new_criteria')\n",
    "            break \n",
    "        old_eigenvalue = eigenvalue\n",
    "        \n",
    "        uniform_rand = torch.rand(proj_Y.shape).to(device)\n",
    "        kept_idx = uniform_rand > (proj_Y/torch.max(proj_Y))\n",
    "        Y = Y[kept_idx]\n",
    "        data = data[kept_idx]\n",
    "    return kept_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1d0dd-87fc-4f55-bb39-61d210848df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "attack fail\n",
      "TRAIN ACC:  0.1304599940776825 TEST ACC:  0.12709999084472656\n",
      "epoch 1\n",
      "attack fail\n",
      "TRAIN ACC:  0.15996000170707703 TEST ACC:  0.1590999960899353\n",
      "epoch 2\n",
      "attack fail\n",
      "TRAIN ACC:  0.18987999856472015 TEST ACC:  0.188400000333786\n",
      "epoch 3\n",
      "attack fail\n",
      "TRAIN ACC:  0.2164199948310852 TEST ACC:  0.21459999680519104\n",
      "epoch 4\n",
      "attack fail\n",
      "TRAIN ACC:  0.240339994430542 TEST ACC:  0.2410999983549118\n",
      "epoch 5\n",
      "attack fail\n",
      "TRAIN ACC:  0.2615799903869629 TEST ACC:  0.26659998297691345\n",
      "epoch 6\n",
      "attack fail\n",
      "TRAIN ACC:  0.280379980802536 TEST ACC:  0.28759998083114624\n",
      "epoch 7\n",
      "attack fail\n",
      "TRAIN ACC:  0.29853999614715576 TEST ACC:  0.3041999936103821\n",
      "epoch 8\n",
      "attack fail\n",
      "TRAIN ACC:  0.3158999979496002 TEST ACC:  0.3203999996185303\n",
      "epoch 9\n",
      "attack fail\n",
      "TRAIN ACC:  0.3316600024700165 TEST ACC:  0.3352999985218048\n",
      "epoch 10\n",
      "attack fail\n",
      "TRAIN ACC:  0.3454200029373169 TEST ACC:  0.34880000352859497\n",
      "epoch 11\n",
      "attack fail\n",
      "TRAIN ACC:  0.3588799834251404 TEST ACC:  0.35929998755455017\n",
      "epoch 12\n",
      "attack fail\n",
      "TRAIN ACC:  0.3705199956893921 TEST ACC:  0.37129998207092285\n",
      "epoch 13\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "n_epoch = 100\n",
    "batch_size = 2048\n",
    "for _ in range(n_epoch):\n",
    "    print(\"epoch \" + str(_))\n",
    "    for i in range(0,len(x_train_poisoned),batch_size):\n",
    "\n",
    "        net_copy = copy.deepcopy(net)\n",
    "        feature = x_train_poisoned[i:i+batch_size]\n",
    "        feature.requires_grad = True  ### CRUCIAL LINE !!!\n",
    "        target = y_train_poisoned[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        output = net_copy(feature)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # removing the outliers\n",
    "        kept_idx = randomized_agg_forced(feature.grad.flatten(start_dim=-1))\n",
    "        # print(kept_idx.shape)\n",
    "        kept_idx = [i for i in range(len(kept_idx)) if kept_idx[i]]\n",
    "        # print(len(feature) - len(kept_idx), \" entries dropped\")\n",
    "        \n",
    "        feature = x_train_poisoned[i:i+batch_size][kept_idx]\n",
    "        target = y_train_poisoned[i:i+batch_size][kept_idx]\n",
    "        optimizer.zero_grad()\n",
    "        output = net(feature)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        pred_class_poison = torch.argmax(net(poisoned_imgs_test), dim=-1)\n",
    "        #adding the [0] here because for some reason torch wont accept a single input. \n",
    "        #poisoned_imgs_test contains two copies of the poisoned image\n",
    "        # print(\"PREDICTED CLASS\"pred_class_poison[0] )\n",
    "        if pred_class_poison[0] == 3:\n",
    "            print(\"attack succeed\")\n",
    "        else:\n",
    "            print(\"attack fail\")\n",
    "\n",
    "        train_acc = torch.sum(torch.argmax(net(x_train), dim=-1) == y_train)/len(y_train)\n",
    "        test_acc = torch.sum(torch.argmax(net(x_test), dim=-1) == y_test)/len(y_test)\n",
    "        print(\"TRAIN ACC: \", float(train_acc), \"TEST ACC: \", float(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490938d-0133-410a-af72-de5f68312ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
